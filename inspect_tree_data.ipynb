{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Tree Training Data\n",
    "\n",
    "Inspect and visualize data loading and pre-processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-11 22:34:32.411921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import imgaug\n",
    "import matplotlib\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Root directory of the project\n",
    "MRCNN_DIR = os.path.abspath(\"../Mask_RCNN/\")\n",
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MRCNN_DIR)\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import utils, visualize\n",
    "from mrcnn.model import log\n",
    "from mrcnn.visualize import display_images\n",
    "\n",
    "import tree\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"data/mrcnn\")\n",
    "\n",
    "\n",
    "# Use configuation from tree.py, but override\n",
    "# image resizing so we see the real sizes here\n",
    "class NoResizeConfig(tree.TreeConfig):\n",
    "    IMAGE_RESIZE_MODE = \"none\"\n",
    "    USE_MINI_MASK = False\n",
    "\n",
    "\n",
    "config = NoResizeConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "\n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size * cols, size * rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the data to better fit with Mask RCNN's assumptions about data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/Users/lusk/projects/uni-potsdam/so22/dap05/proj/tree/data/mrcnn_imp/393_5823_RGB_2020_01_00/image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 21\u001b[0m\n\u001b[1;32m     16\u001b[0m label_ids\u001b[39m.\u001b[39msort()\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i, img \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(image_ids):\n\u001b[1;32m     19\u001b[0m     \u001b[39m# random_bits = random.getrandbits(128)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# hash1 = \"%032x\" % random_bits\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATASET_DIR\u001b[39m}\u001b[39;00m\u001b[39m/mrcnn_imp/\u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m}\u001b[39;00m\u001b[39m/image\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATASET_DIR\u001b[39m}\u001b[39;00m\u001b[39m/mrcnn_imp/\u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m}\u001b[39;00m\u001b[39m/mask\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     shutil\u001b[39m.\u001b[39mcopyfile(\n\u001b[1;32m     24\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATASET_DIR\u001b[39m}\u001b[39;00m\u001b[39m/rgb/\u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m}\u001b[39;00m\u001b[39m.tif\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mDATASET_DIR\u001b[39m}\u001b[39;00m\u001b[39m/mrcnn_imp/\u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m}\u001b[39;00m\u001b[39m/image/\u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m}\u001b[39;00m\u001b[39m.tif\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dap05/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     mkdir(name, mode)\n\u001b[1;32m    226\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[39m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[39m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exist_ok \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/Users/lusk/projects/uni-potsdam/so22/dap05/proj/tree/data/mrcnn_imp/393_5823_RGB_2020_01_00/image'"
     ]
    }
   ],
   "source": [
    "organize_data = True\n",
    "\n",
    "if organize_data:\n",
    "    import shutil\n",
    "\n",
    "    DATASET_DIR = os.path.join(ROOT_DIR, \"data\")\n",
    "    rgb_dir = os.path.join(DATASET_DIR, \"rgb_imp\")\n",
    "    label_dir = os.path.join(DATASET_DIR, \"label_imp\")\n",
    "\n",
    "    image_ids = os.listdir(rgb_dir)\n",
    "    image_ids = [x.split(\".\")[0] for x in image_ids]\n",
    "    label_ids = os.listdir(label_dir)\n",
    "    label_ids = [x.split(\".\")[0] for x in label_ids]\n",
    "\n",
    "    image_ids.sort()\n",
    "    label_ids.sort()\n",
    "\n",
    "    for i, img in enumerate(image_ids):\n",
    "        # random_bits = random.getrandbits(128)\n",
    "        # hash1 = \"%032x\" % random_bits\n",
    "        os.makedirs(f\"{DATASET_DIR}/mrcnn_imp/{img}/image\")\n",
    "        os.makedirs(f\"{DATASET_DIR}/mrcnn_imp/{img}/mask\")\n",
    "        shutil.copyfile(\n",
    "            f\"{DATASET_DIR}/rgb/{img}.tif\", f\"{DATASET_DIR}/mrcnn_imp/{img}/image/{img}.tif\"\n",
    "        )\n",
    "        shutil.copyfile(\n",
    "            f\"{label_dir}/{label_ids[i]}.tif\",\n",
    "            f\"{DATASET_DIR}/mrcnn_imp/{img}/mask/{label_ids[i]}.tif\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = tree.TreeDataset()\n",
    "\n",
    "dataset.load_tree(DATASET_DIR, subset=\"hand\", split=0.1)\n",
    "\n",
    "# Must call before using the dataset\n",
    "dataset.prepare()\n",
    "\n",
    "print(\"Image Count: {}\".format(len(dataset.image_ids)))\n",
    "print(\"Class Count: {}\".format(dataset.num_classes))\n",
    "for i, info in enumerate(dataset.class_info):\n",
    "    print(\"{:3}. {:50}\".format(i, info[\"name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.image_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a specific image by its source ID\n",
    "source_id = \"393_5823_RGB_2020_01_03\"\n",
    "\n",
    "# Map source ID to Dataset image_id\n",
    "# Notice the tree prefix: it's the name given to the dataset in TreeDataset\n",
    "image_id = dataset.image_from_source_map[\"tree.{}\".format(source_id)]\n",
    "\n",
    "# Load and display\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id\n",
    ")\n",
    "log(\"molded_image\", image)\n",
    "log(\"mask\", mask)\n",
    "visualize.display_instances(\n",
    "    image, bbox, mask, class_ids, dataset.class_names, show_bbox=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Stats\n",
    "\n",
    "Loop through all images in the dataset and collect aggregate stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_stats(image_id):\n",
    "    \"\"\"Returns a dict of stats for one image.\"\"\"\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, _ = dataset.load_mask(image_id)\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "    # Sanity check\n",
    "    assert mask.shape[:2] == image.shape[:2]\n",
    "    # Return stats dict\n",
    "    return {\n",
    "        \"id\": image_id,\n",
    "        \"shape\": list(image.shape),\n",
    "        \"bbox\": [\n",
    "            [b[2] - b[0], b[3] - b[1]]\n",
    "            for b in bbox\n",
    "            # Uncomment to exclude trees with 1 pixel width\n",
    "            # or height (often on edges)\n",
    "            # if b[2] - b[0] > 1 and b[3] - b[1] > 1\n",
    "        ],\n",
    "        \"color\": np.mean(image, axis=(0, 1)),\n",
    "    }\n",
    "\n",
    "\n",
    "# Loop through the dataset and compute stats over multiple threads\n",
    "# This might take a few minutes\n",
    "t_start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor() as e:\n",
    "    stats = list(e.map(image_stats, dataset.image_ids))\n",
    "t_total = time.time() - t_start\n",
    "print(\"Total time: {:.1f} seconds\".format(t_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Size Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image stats\n",
    "image_shape = np.array([s[\"shape\"] for s in stats])\n",
    "image_color = np.array([s[\"color\"] for s in stats])\n",
    "print(\"Image Count: \", image_shape.shape[0])\n",
    "print(\n",
    "    \"Height  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(image_shape[:, 0]),\n",
    "        np.median(image_shape[:, 0]),\n",
    "        np.min(image_shape[:, 0]),\n",
    "        np.max(image_shape[:, 0]),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Width   mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(image_shape[:, 1]),\n",
    "        np.median(image_shape[:, 1]),\n",
    "        np.min(image_shape[:, 1]),\n",
    "        np.max(image_shape[:, 1]),\n",
    "    )\n",
    ")\n",
    "print(\"Color   mean (RGB): {:.2f} {:.2f} {:.2f}\".format(*np.mean(image_color, axis=0)))\n",
    "\n",
    "# Histograms\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 4))\n",
    "ax[0].set_title(\"Height\")\n",
    "_ = ax[0].hist(image_shape[:, 0], bins=20)\n",
    "ax[1].set_title(\"Width\")\n",
    "_ = ax[1].hist(image_shape[:, 1], bins=20)\n",
    "ax[2].set_title(\"Height & Width\")\n",
    "_ = ax[2].hist2d(image_shape[:, 1], image_shape[:, 0], bins=10, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trees per Image Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_area = 512**2\n",
    "\n",
    "print(\"Tree/Image\")\n",
    "fig, ax = plt.subplots(1)\n",
    "area_threshold = 0\n",
    "\n",
    "tree_per_image = np.array(\n",
    "    [\n",
    "        len(s[\"bbox\"])\n",
    "        for s in stats\n",
    "        if area_threshold < (s[\"shape\"][0] * s[\"shape\"][1]) <= image_area\n",
    "    ]\n",
    ")\n",
    "area_threshold = image_area\n",
    "if len(tree_per_image) == 0:\n",
    "    print(\"Image area <= {:4}**2: None\".format(np.sqrt(image_area)))\n",
    "\n",
    "print(\n",
    "    \"Image area = {:4.0f}**2:  mean: {:.1f}  median: {:.1f}  min: {:.1f}  max: {:.1f}\".format(\n",
    "        np.sqrt(image_area),\n",
    "        tree_per_image.mean(),\n",
    "        np.median(tree_per_image),\n",
    "        tree_per_image.min(),\n",
    "        tree_per_image.max(),\n",
    "    )\n",
    ")\n",
    "ax.set_title(\"Image Area = {:4}**2\".format(np.sqrt(image_area)))\n",
    "_ = ax.hist(tree_per_image, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Size Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees size stats\n",
    "fig, ax = plt.subplots(1)\n",
    "area_threshold = 0\n",
    "\n",
    "tree_shape = np.array(\n",
    "    [\n",
    "        b\n",
    "        for s in stats\n",
    "        if area_threshold < (s[\"shape\"][0] * s[\"shape\"][1]) <= image_area\n",
    "        for b in s[\"bbox\"]\n",
    "    ]\n",
    ")\n",
    "tree_area = tree_shape[:, 0] * tree_shape[:, 1]\n",
    "area_threshold = image_area\n",
    "\n",
    "print(\"\\nImage Area = {:.0f}**2\".format(np.sqrt(image_area)))\n",
    "print(\"  Total Trees: \", tree_shape.shape[0])\n",
    "print(\n",
    "    \"  Tree Height. mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(tree_shape[:, 0]),\n",
    "        np.median(tree_shape[:, 0]),\n",
    "        np.min(tree_shape[:, 0]),\n",
    "        np.max(tree_shape[:, 0]),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"  Tree Width.  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(tree_shape[:, 1]),\n",
    "        np.median(tree_shape[:, 1]),\n",
    "        np.min(tree_shape[:, 1]),\n",
    "        np.max(tree_shape[:, 1]),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"  Tree Area.   mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(tree_area),\n",
    "        np.median(tree_area),\n",
    "        np.min(tree_area),\n",
    "        np.max(tree_area),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show 2D histogram\n",
    "_ = ax.hist2d(tree_shape[:, 1], tree_shape[:, 0], bins=20, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees height/width ratio\n",
    "tree_aspect_ratio = tree_shape[:, 0] / tree_shape[:, 1]\n",
    "print(\n",
    "    \"Tree Aspect Ratio.  mean: {:.2f}  median: {:.2f}  min: {:.2f}  max: {:.2f}\".format(\n",
    "        np.mean(tree_aspect_ratio),\n",
    "        np.median(tree_aspect_ratio),\n",
    "        np.min(tree_aspect_ratio),\n",
    "        np.max(tree_aspect_ratio),\n",
    "    )\n",
    ")\n",
    "plt.figure(figsize=(15, 5))\n",
    "_ = plt.hist(tree_aspect_ratio, bins=100, range=[0, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "Test out different augmentation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of augmentations\n",
    "# http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "augmentation = iaa.Sometimes(\n",
    "    0.9,\n",
    "    [\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Flipud(0.5),\n",
    "        iaa.Multiply((0.8, 1.2)),\n",
    "        iaa.GaussianBlur(sigma=(0.0, 5.0)),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image multiple times to show augmentations\n",
    "limit = 4\n",
    "ax = get_ax(rows=2, cols=limit // 2)\n",
    "for i in range(limit):\n",
    "    image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset,\n",
    "        config,\n",
    "        image_id,\n",
    "        augmentation=augmentation,\n",
    "    )\n",
    "    visualize.display_instances(\n",
    "        image,\n",
    "        bbox,\n",
    "        mask,\n",
    "        class_ids,\n",
    "        dataset.class_names,\n",
    "        ax=ax[i // 2, i % 2],\n",
    "        show_mask=False,\n",
    "        show_bbox=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Crops\n",
    "\n",
    "Aerial images tend to be large, but trees are small. So it's more efficient to train on random crops from large images. This is handled by `config.IMAGE_RESIZE_MODE = \"crop\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropConfig(tree.TreeConfig):\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "\n",
    "crop_config = RandomCropConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image multiple times to show different crops\n",
    "limit = 4\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "ax = get_ax(rows=2, cols=limit // 2)\n",
    "for i in range(limit):\n",
    "    image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "        dataset, crop_config, image_id\n",
    "    )\n",
    "    visualize.display_instances(\n",
    "        image,\n",
    "        bbox,\n",
    "        mask,\n",
    "        class_ids,\n",
    "        dataset.class_names,\n",
    "        ax=ax[i // 2, i % 2],\n",
    "        show_mask=False,\n",
    "        show_bbox=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Masks\n",
    "\n",
    "Instance binary masks can get large when training with high resolution images. For example, if training with 1024x1024 image then the mask of a single instance requires 1MB of memory (Numpy uses bytes for boolean values). If an image has 100 instances then that's 100MB for the masks alone. \n",
    "\n",
    "To improve training speed, we optimize masks:\n",
    "* We store mask pixels that are inside the object bounding box, rather than a mask of the full image. Most objects are small compared to the image size, so we save space by not storing a lot of zeros around the object.\n",
    "* We resize the mask to a smaller size (e.g. 56x56). For objects that are larger than the selected size we lose a bit of accuracy. But most object annotations are not very accuracy to begin with, so this loss is negligable for most practical purposes. Thie size of the mini_mask can be set in the config class.\n",
    "\n",
    "To visualize the effect of mask resizing, and to verify the code correctness, we visualize some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load random image and mask.\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image = dataset.load_image(image_id)\n",
    "mask, class_ids = dataset.load_mask(image_id)\n",
    "original_shape = image.shape\n",
    "# Resize\n",
    "image, window, scale, padding, _ = utils.resize_image(\n",
    "    image,\n",
    "    min_dim=config.IMAGE_MIN_DIM,\n",
    "    max_dim=config.IMAGE_MAX_DIM,\n",
    "    mode=config.IMAGE_RESIZE_MODE,\n",
    ")\n",
    "mask = utils.resize_mask(mask, scale, padding)\n",
    "# Compute Bounding box\n",
    "bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "# Display image and additional stats\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "print(\"Original shape: \", original_shape)\n",
    "log(\"image\", image)\n",
    "log(\"mask\", mask)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "# Display image and instances\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, config, image_id\n",
    ")\n",
    "\n",
    "log(\"image\", image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "log(\"mask\", mask)\n",
    "\n",
    "display_images([image] + [mask[:, :, i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMaskConfig(RandomCropConfig):\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "\n",
    "mm_config = MiniMaskConfig()\n",
    "\n",
    "# Add augmentation and mask resizing.\n",
    "image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(\n",
    "    dataset, mm_config, image_id, augmentation=augmentation\n",
    ")\n",
    "log(\"mask\", mask)\n",
    "display_images([image] + [mask[:, :, i] for i in range(min(mask.shape[-1], 7))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = utils.expand_mask(bbox, mask, image.shape)\n",
    "visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchors\n",
    "\n",
    "For an FPN network, the anchors must be ordered in a way that makes it easy to match anchors to the output of the convolution layers that predict anchor scores and shifts. \n",
    "* Sort by pyramid level first. All anchors of the first level, then all of the second and so on. This makes it easier to separate anchors by level.\n",
    "* Within each level, sort anchors by feature map processing sequence. Typically, a convolution layer processes a feature map starting from top-left and moving right row by row. \n",
    "* For each feature map cell, pick any sorting order for the anchors of different ratios. Here we match the order of ratios passed to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize anchors of one cell at the center of the feature map\n",
    "\n",
    "# Load and display random image\n",
    "image_id = np.random.choice(dataset.image_ids, 1)[0]\n",
    "image, image_meta, _, _, _ = modellib.load_image_gt(dataset, crop_config, image_id)\n",
    "\n",
    "# Generate Anchors\n",
    "backbone_shapes = modellib.compute_backbone_shapes(config, image.shape)\n",
    "anchors = utils.generate_pyramid_anchors(\n",
    "    config.RPN_ANCHOR_SCALES,\n",
    "    config.RPN_ANCHOR_RATIOS,\n",
    "    backbone_shapes,\n",
    "    config.BACKBONE_STRIDES,\n",
    "    config.RPN_ANCHOR_STRIDE,\n",
    ")\n",
    "\n",
    "# Print summary of anchors\n",
    "num_levels = len(backbone_shapes)\n",
    "anchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Count: \", anchors.shape[0])\n",
    "print(\"Scales: \", config.RPN_ANCHOR_SCALES)\n",
    "print(\"ratios: \", config.RPN_ANCHOR_RATIOS)\n",
    "print(\"Anchors per Cell: \", anchors_per_cell)\n",
    "print(\"Levels: \", num_levels)\n",
    "anchors_per_level = []\n",
    "for l in range(num_levels):\n",
    "    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n",
    "    anchors_per_level.append(\n",
    "        anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2\n",
    "    )\n",
    "    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))\n",
    "\n",
    "# Display\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "ax.imshow(image)\n",
    "levels = len(backbone_shapes)\n",
    "\n",
    "for level in range(levels):\n",
    "    colors = visualize.random_colors(levels)\n",
    "    # Compute the index of the anchors at the center of the image\n",
    "    level_start = sum(anchors_per_level[:level])  # sum of anchors of previous levels\n",
    "    level_anchors = anchors[level_start : level_start + anchors_per_level[level]]\n",
    "    print(\n",
    "        \"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(\n",
    "            level, level_anchors.shape[0], backbone_shapes[level]\n",
    "        )\n",
    "    )\n",
    "    center_cell = backbone_shapes[level] // 2\n",
    "    center_cell_index = center_cell[0] * backbone_shapes[level][1] + center_cell[1]\n",
    "    level_center = center_cell_index * anchors_per_cell\n",
    "    center_anchor = anchors_per_cell * (\n",
    "        (center_cell[0] * backbone_shapes[level][1] / config.RPN_ANCHOR_STRIDE**2)\n",
    "        + center_cell[1] / config.RPN_ANCHOR_STRIDE\n",
    "    )\n",
    "    level_center = int(center_anchor)\n",
    "\n",
    "    # Draw anchors. Brightness show the order in the array, dark to bright.\n",
    "    for i, rect in enumerate(\n",
    "        level_anchors[level_center : level_center + anchors_per_cell]\n",
    "    ):\n",
    "        y1, x1, y2, x2 = rect\n",
    "        p = patches.Rectangle(\n",
    "            (x1, y1),\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            linewidth=2,\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=(i + 1) * np.array(colors[level]) / anchors_per_cell,\n",
    "        )\n",
    "        ax.add_patch(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rois = 2000\n",
    "\n",
    "g = modellib.DataGenerator(\n",
    "    dataset,\n",
    "    mm_config,\n",
    "    augmentation=augmentation,\n",
    "    random_rois=random_rois,\n",
    "    detection_targets=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data generator\n",
    "# random_rois = 2000\n",
    "# g = modellib.data_generator(\n",
    "#     dataset,\n",
    "#     crop_config,\n",
    "#     shuffle=True,\n",
    "#     random_rois=random_rois,\n",
    "#     batch_size=4,\n",
    "#     detection_targets=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to run the generator through a lot of images\n",
    "# to catch rare errors\n",
    "\n",
    "# Ignore the imgaug warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for i in range(1000):\n",
    "    print(i)\n",
    "    _, _ = g[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Next Image\n",
    "if random_rois:\n",
    "    [\n",
    "        normalized_images,\n",
    "        image_meta,\n",
    "        rpn_match,\n",
    "        rpn_bbox,\n",
    "        gt_class_ids,\n",
    "        gt_boxes,\n",
    "        gt_masks,\n",
    "        rpn_rois,\n",
    "        rois,\n",
    "    ], [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(iter(g))\n",
    "\n",
    "    log(\"rois\", rois)\n",
    "    log(\"mrcnn_class_ids\", mrcnn_class_ids)\n",
    "    log(\"mrcnn_bbox\", mrcnn_bbox)\n",
    "    log(\"mrcnn_mask\", mrcnn_mask)\n",
    "else:\n",
    "    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(\n",
    "        iter(g)\n",
    "    )\n",
    "\n",
    "log(\"gt_class_ids\", gt_class_ids)\n",
    "log(\"gt_boxes\", gt_boxes)\n",
    "log(\"gt_masks\", gt_masks)\n",
    "log(\n",
    "    \"rpn_match\",\n",
    "    rpn_match,\n",
    ")\n",
    "log(\"rpn_bbox\", rpn_bbox)\n",
    "image_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n",
    "print(\"image_id: \", image_id, dataset.image_reference(image_id))\n",
    "\n",
    "# Remove the last dim in mrcnn_class_ids. It's only added\n",
    "# to satisfy Keras restriction on target shape.\n",
    "mrcnn_class_ids = mrcnn_class_ids[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "\n",
    "# Restore original image (reverse normalization)\n",
    "sample_image = modellib.unmold_image(normalized_images[b], config)\n",
    "\n",
    "# Compute anchor shifts.\n",
    "indices = np.where(rpn_match[b] == 1)[0]\n",
    "refined_anchors = utils.apply_box_deltas(\n",
    "    anchors[indices], rpn_bbox[b, : len(indices)] * config.RPN_BBOX_STD_DEV\n",
    ")\n",
    "log(\"anchors\", anchors)\n",
    "log(\"refined_anchors\", refined_anchors)\n",
    "\n",
    "# Get list of positive anchors\n",
    "positive_anchor_ids = np.where(rpn_match[b] == 1)[0]\n",
    "print(\"Positive anchors: {}\".format(len(positive_anchor_ids)))\n",
    "negative_anchor_ids = np.where(rpn_match[b] == -1)[0]\n",
    "print(\"Negative anchors: {}\".format(len(negative_anchor_ids)))\n",
    "neutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n",
    "print(\"Neutral anchors: {}\".format(len(neutral_anchor_ids)))\n",
    "\n",
    "# ROI breakdown by class\n",
    "for c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n",
    "    if n:\n",
    "        print(\"{:23}: {}\".format(c[:20], n))\n",
    "\n",
    "# Show positive anchors\n",
    "fig, ax = plt.subplots(1, figsize=(16, 16))\n",
    "visualize.draw_boxes(\n",
    "    sample_image,\n",
    "    boxes=anchors[positive_anchor_ids],\n",
    "    refined_boxes=refined_anchors,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show negative anchors\n",
    "visualize.draw_boxes(sample_image, boxes=anchors[negative_anchor_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show neutral anchors. They don't contribute to training.\n",
    "visualize.draw_boxes(\n",
    "    sample_image, boxes=anchors[np.random.choice(neutral_anchor_ids, 100)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROIs\n",
    "\n",
    "Typically, the RPN network generates region proposals (a.k.a. Regions of Interest, or ROIs). The data generator has the ability to generate proposals as well for illustration and testing purposes. These are controlled by the `random_rois` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_rois:\n",
    "    # Class aware bboxes\n",
    "    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n",
    "\n",
    "    # Refined ROIs\n",
    "    refined_rois = utils.apply_box_deltas(\n",
    "        rois[b].astype(np.float32), bbox_specific[:, :4] * config.BBOX_STD_DEV\n",
    "    )\n",
    "\n",
    "    # Class aware masks\n",
    "    mask_specific = mrcnn_mask[\n",
    "        b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]\n",
    "    ]\n",
    "\n",
    "    visualize.draw_rois(\n",
    "        sample_image,\n",
    "        rois[b],\n",
    "        refined_rois,\n",
    "        mask_specific,\n",
    "        mrcnn_class_ids[b],\n",
    "        dataset.class_names,\n",
    "    )\n",
    "\n",
    "    # Any repeated ROIs?\n",
    "    rows = np.ascontiguousarray(rois[b]).view(\n",
    "        np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1]))\n",
    "    )\n",
    "    _, idx = np.unique(rows, return_index=True)\n",
    "    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if random_rois:\n",
    "    # Dispalay ROIs and corresponding masks and bounding boxes\n",
    "    ids = random.sample(range(rois.shape[1]), 8)\n",
    "\n",
    "    images = []\n",
    "    titles = []\n",
    "    for i in ids:\n",
    "        image = visualize.draw_box(\n",
    "            sample_image.copy(), rois[b, i, :4].astype(np.int32), [255, 0, 0]\n",
    "        )\n",
    "        image = visualize.draw_box(image, refined_rois[i].astype(np.int64), [0, 255, 0])\n",
    "        images.append(image)\n",
    "        titles.append(\"ROI {}\".format(i))\n",
    "        images.append(mask_specific[i] * 255)\n",
    "        titles.append(dataset.class_names[mrcnn_class_ids[b, i]][:20])\n",
    "\n",
    "    display_images(images, titles, cols=4, cmap=\"Blues\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: For some reason temp_g is returning the same exact ids for each batch..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ratio of positive ROIs in a set of images.\n",
    "if random_rois:\n",
    "    limit = 10\n",
    "    temp_g = modellib.DataGenerator(\n",
    "        dataset,\n",
    "        mm_config,\n",
    "        shuffle=True,\n",
    "        augmentation=augmentation,\n",
    "        random_rois=10000,\n",
    "        detection_targets=True,\n",
    "    )\n",
    "    total = 0\n",
    "    inputses = []\n",
    "    idses = []\n",
    "    for i in range(limit):\n",
    "        inputs, [ids, _, _] = temp_g[i]\n",
    "        inputses.append(inputs)\n",
    "        idses.append(ids)\n",
    "        if i > 0 and (np.max(idses[i] - idses[i - 1]) == 0):\n",
    "            print(\"ids are the same\")\n",
    "        positive_rois = np.sum(ids[0] > 0)\n",
    "        total += positive_rois\n",
    "        print(\"{:5} {:5.2f}\".format(positive_rois, positive_rois / ids.shape[1]))\n",
    "    print(\"Average percent: {:.2f}\".format(total / (limit * ids.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(idses[0][0] - idses[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(temp_g[0][1][0] - temp_g[3][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_g[1][0][2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf136276df7b73f579d00ffc03546cf27d68eba258d4f2ea564dd7fb6a9dd1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
