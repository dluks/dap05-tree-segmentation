{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "278a6d15-2b2b-4998-a19b-ac114ed0ce99",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Comparing instance segmentation of trees and watershed-based instance segmentation of semantically segmented trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1c3865-3d4a-4ccb-8c91-d74a393641c9",
   "metadata": {},
   "source": [
    "## To-dos\n",
    "- **Perform grid search to determine best hyperparameters (batch size, epochs, learning rate)**\n",
    "    - Don't forget to also incorporate KFolds CV!\n",
    "- **Improve quality of watershed labels**\n",
    "    - Review the labels and remove non-tree labels\n",
    "- **Generate boundaries on label data**\n",
    "    - Only applicable if using the U-Net approach (I think?)\n",
    "    - Consider using a positive boundary (dilate) and a negative (erode) and compare\n",
    "- **Incorporate KFolds (10 folds) cross-validation into model training**\n",
    "    - I.e. Leave a test set alone and then use KFolds on the training set to derive train/val subsets)\n",
    "- **Consider the Mask RCNN approach: https://github.com/matterport/Mask_RCNN**\n",
    "   \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ee39e-760c-4c0f-909e-f5724138ea3d",
   "metadata": {},
   "source": [
    "## Instance segmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6763067-39be-4621-a43a-6890cdb45898",
   "metadata": {},
   "source": [
    "### Imports and function defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7823f-368b-494e-b6f0-26a0a49d2d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from patchify import patchify\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "\n",
    "def patch_train_label(raster, labels, img_size, channels=False, merge_channel=False):\n",
    "    samp_rast = tiff.imread(raster[0])\n",
    "    img_base_size = samp_rast.shape[0]\n",
    "    n = len(raster)\n",
    "    m = (img_base_size // img_size) ** 2\n",
    "\n",
    "    if not channels:\n",
    "        channels = samp_rast.shape[-1]\n",
    "\n",
    "    if merge_channel:\n",
    "        channels += tiff.imread(merge_channel[0]).shape[-1]\n",
    "\n",
    "    data_train = np.zeros((n * m, img_size, img_size, channels))\n",
    "    data_label = np.zeros((n * m, img_size, img_size))\n",
    "\n",
    "    for k in range(n):\n",
    "        if merge_channel:\n",
    "            r = np.concatenate(\n",
    "                (tiff.imread(raster[k]), tiff.imread(merge_channel[k])), axis=-1\n",
    "            )\n",
    "        else:\n",
    "            r = tiff.imread(raster[k])[..., :channels]\n",
    "\n",
    "        # Only read in the specified number of channels from input raster\n",
    "        patches_train = patchify(\n",
    "            r,\n",
    "            (img_size, img_size, channels),\n",
    "            step=img_size,\n",
    "        )\n",
    "        patches_label = patchify(\n",
    "            tiff.imread(labels[k]), (img_size, img_size), step=img_size\n",
    "        )\n",
    "        data_train[k * m : (k + 1) * m, :, :, :] = patches_train.reshape(\n",
    "            -1, img_size, img_size, channels\n",
    "        )\n",
    "        data_label[k * m : (k + 1) * m, :, :] = patches_label.reshape(\n",
    "            -1, img_size, img_size\n",
    "        )\n",
    "\n",
    "    data_label = (data_label > 0).astype(\"int\")\n",
    "    data_label = np.expand_dims(data_label, axis=-1)\n",
    "    data_train = data_train.astype(\"float\") / 255\n",
    "\n",
    "    print(\n",
    "        f\"\\nData sizes:\\ndata_train: {data_train.shape}\\ndata_label: {data_label.shape}\\n\"\n",
    "    )\n",
    "\n",
    "    return data_train, data_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6696e7d-8633-44e0-802d-f6be59bd0da6",
   "metadata": {},
   "source": [
    "### Load, patchify, and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f2a2ea-2863-4e62-ad3f-23daa79494b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patchify hand-labeled data PLUS NIR data\n",
    "data_dir = \"../data/\"\n",
    "hand_rgb_dir = f\"{data_dir}train_rgb/\"\n",
    "hand_nir_dir = f\"{data_dir}train_nir/\"\n",
    "hand_label_dir = f\"{data_dir}label/\"\n",
    "\n",
    "patch_rgb = glob.glob(f\"{hand_rgb_dir}*.tif\")\n",
    "patch_nir = glob.glob(f\"{hand_nir_dir}*.tif\")\n",
    "patch_label = glob.glob(f\"{hand_label_dir}*.tif\")\n",
    "patch_rgb.sort()\n",
    "patch_label.sort()\n",
    "\n",
    "print(\"Patchifying RGB + NIR data...\")\n",
    "data_train, data_label = patch_train_label(\n",
    "    patch_rgb, patch_label, 128, merge_channel=patch_nir\n",
    ")\n",
    "\n",
    "# Patchify watershed data (pre-patchified)\n",
    "patched_watershed_rgbi_dir = f\"{data_dir}watershed/512/rgbi/\"\n",
    "patched_watershed_label_dir = f\"{data_dir}watershed/512/labels/\"\n",
    "\n",
    "watershed_rgbi = glob.glob(f\"{patched_watershed_rgbi_dir}*.tif\")\n",
    "watershed_labels = glob.glob(f\"{patched_watershed_label_dir}*.tif\")\n",
    "watershed_rgbi.sort()\n",
    "watershed_labels.sort()\n",
    "\n",
    "print(\"Patchifying watershed data...\")\n",
    "data_train_ws, data_label_ws = patch_train_label(watershed_rgbi, watershed_labels, 128)\n",
    "\n",
    "data_train = np.vstack((data_train, data_train_ws))\n",
    "data_label = np.vstack((data_label, data_label_ws))\n",
    "\n",
    "print(\n",
    "    f\"\\nSizes after adding watershed data:\\n\\\n",
    "data_train: {data_train.shape}\\n\\\n",
    "data_label: {data_label.shape}\\n\"\n",
    ")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_train, data_label, test_size=0.1, random_state=157\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nSizes after splitting data:\\n\\\n",
    "x_train: {x_train.shape}\\n\\\n",
    "y_train: {y_train.shape}\\n\\\n",
    "x_test: {x_test.shape}\\n\\\n",
    "y_test: {y_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda8a901-da6d-4a7b-8468-eb7a044120e5",
   "metadata": {},
   "source": [
    "## Messing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252b54f4-0996-4a7f-abfe-dfd7a9e697bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_fn = glob.glob(\"../data/watershed/512/labels/*.tif\")\n",
    "im = tiff.imread(im_fn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d9062a-63ca-45b3-88bd-e92a9a7ea014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb5f57-be0b-40f6-a5be-558855f48412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "im.astype(\"float\")\n",
    "im[im == 0] = np.nan\n",
    "plt.imshow(im, cmap=plt.cm.tab20c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64c30c-634a-421f-af0c-bfd881129db1",
   "metadata": {},
   "source": [
    "## Semantic watershed segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c017ec0-2275-4db4-9363-9b287af28775",
   "metadata": {},
   "source": [
    "## Mask R-CNN Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b84b520-646d-49ac-820f-d89c21624bb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 08 Oct, 2022\n",
    "---\n",
    "\n",
    "#### Run 1 (well, first run taking notes)\n",
    "\n",
    "- Full dataset\n",
    "- Split 0.1\n",
    "- Seed = 1009\n",
    "- Weights: Coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e8453-7616-4599-aed5-c80069d01e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config2(tree.TreeConfig):\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # Reduce validation steps because the epoch is also reduced\n",
    "    VALIDATION_STEPS = 10\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + tree\n",
    "    \n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128)\n",
    "    \n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more proposals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([107.0, 105.2, 101.5])\n",
    "    \n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "    \n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 101\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between tree and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = False\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    \n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.005\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.0,\n",
    "        \"rpn_bbox_loss\": 1.0,\n",
    "        \"mrcnn_class_loss\": 1.0,\n",
    "        \"mrcnn_bbox_loss\": 1.0,\n",
    "        \"mrcnn_mask_loss\": 1.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5637afa0-1963-4b38-b4f9-c9e64df7cc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training schedule\n",
    "\n",
    "print(\"Train network heads\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=20,\n",
    "            augmentation=augmentation,\n",
    "            layers='heads')\n",
    "# Finetune layers from ResNet stage 4 and up\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=40,\n",
    "            layers='4+')\n",
    "\n",
    "print(\"Train all layers\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE/10,\n",
    "            epochs=60,\n",
    "            augmentation=augmentation,\n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786292c",
   "metadata": {},
   "source": [
    "### 09 Oct, 2022\n",
    "---\n",
    "\n",
    "**Goals**\n",
    "1. Explore yesterday's run\n",
    "2. Decide if continuing with M-RCNN is worth it\n",
    "3. Develop approach for applying watershed to semantic seg output\n",
    "4. (optional) Explore GridSearch approach to optimizing semi-automatic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350ca48",
   "metadata": {},
   "source": [
    "#### ~~Run 2~~ Canceled because I realized I set the steps per epoch way too high.\n",
    "\n",
    "- Full dataset\n",
    "- Split 0.1\n",
    "- Seed = 1009\n",
    "- Weights: Coco\n",
    "\n",
    "##### Config\n",
    "\n",
    "**Config Changes**\n",
    "- Increase steps per epoch from 100 to 500\n",
    "- Increase validation steps from 10 to 50\n",
    "\n",
    "```python\n",
    "class TreeConfig(Config):\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 500\n",
    "\n",
    "    # Reduce validation steps because the epoch is also reduced\n",
    "    VALIDATION_STEPS = 100\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + tree\n",
    "    \n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128)\n",
    "    \n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more proposals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([107.0, 105.2, 101.5])\n",
    "    \n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "    \n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 101\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between tree and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = False\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    \n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.005\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.0,\n",
    "        \"rpn_bbox_loss\": 1.0,\n",
    "        \"mrcnn_class_loss\": 1.0,\n",
    "        \"mrcnn_bbox_loss\": 1.0,\n",
    "        \"mrcnn_mask_loss\": 1.0,\n",
    "    }\n",
    "```\n",
    "\n",
    "##### Training Schedule\n",
    "\n",
    "**Training schedule changes**\n",
    "- Reduce Gaussian blur from 5.0 to 1.0\n",
    "- Increased steps per epoch from config\n",
    "```python\n",
    "# Image augmentation\n",
    "    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "    if augmentation:\n",
    "        augmentation = iaa.SomeOf(\n",
    "            (0, 2),\n",
    "            [\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "                iaa.OneOf(\n",
    "                    [\n",
    "                        iaa.Affine(rotate=90),\n",
    "                        iaa.Affine(rotate=180),\n",
    "                        iaa.Affine(rotate=270),\n",
    "                    ]\n",
    "                ),\n",
    "                iaa.Multiply((0.8, 1.5)),\n",
    "                iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    print(\"Train network heads\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=20,\n",
    "                augmentation=augmentation,\n",
    "                layers='heads')\n",
    "    # Finetune layers from ResNet stage 4 and up\n",
    "    print(\"Fine tune Resnet stage 4 and up\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=40,\n",
    "                layers='4+')\n",
    "\n",
    "    print(\"Train all layers\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE/10,\n",
    "                epochs=60,\n",
    "                augmentation=augmentation,\n",
    "                layers='all')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415368a8",
   "metadata": {},
   "source": [
    "#### Run 3\n",
    "\n",
    "- Full dataset\n",
    "- Split 0.1\n",
    "- Seed = 1009\n",
    "- Weights: Coco\n",
    "- Augumentation = True\n",
    "\n",
    "##### Config\n",
    "\n",
    "**Config Changes**\n",
    "- Update steps/val steps per epoch to represent dataset size\n",
    "- Use Resnet50\n",
    "\n",
    "```python\n",
    "class TreeConfig(Config):\n",
    "\n",
    "    # Number of images to train with on each GPU. A 12GB GPU can typically\n",
    "    # handle 2 images of 1024x1024px.\n",
    "    # Adjust based on your GPU memory and image sizes. Use the highest\n",
    "    # number that your GPU can handle for best performance.\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    # This doesn't need to match the size of the training set. Tensorboard\n",
    "    # updates are saved at the end of each epoch, so setting this to a\n",
    "    # smaller number means getting more frequent TensorBoard updates.\n",
    "    # Validation stats are also calculated at each epoch end and they\n",
    "    # might take a while, so don't set this too small to avoid spending\n",
    "    # a lot of time on validation stats.\n",
    "    STEPS_PER_EPOCH = 324 // IMAGES_PER_GPU\n",
    "    VALIDATION_STEPS = 36 // IMAGES_PER_GPU\n",
    "\n",
    "    # Number of classification classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + tree\n",
    "    \n",
    "    TRAIN_ROIS_PER_IMAGE = 200\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128)\n",
    "    \n",
    "    # Ratios of anchors at each cell (width/height)\n",
    "    # A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "    RPN_ANCHOR_RATIOS = [0.5, 1, 1.5]\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more proposals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet50\"\n",
    "\n",
    "    # Input image resizing\n",
    "    # Generally, use the \"square\" resizing mode for training and predicting\n",
    "    # and it should work well in most cases. In this mode, images are scaled\n",
    "    # up such that the small side is = IMAGE_MIN_DIM, but ensuring that the\n",
    "    # scaling doesn't make the long side > IMAGE_MAX_DIM. Then the image is\n",
    "    # padded with zeros to make it a square so multiple images can be put\n",
    "    # in one batch.\n",
    "    # Available resizing modes:\n",
    "    # none:   No resizing or padding. Return the image unchanged.\n",
    "    # square: Resize and pad with zeros to get a square image\n",
    "    #         of size [max_dim, max_dim].\n",
    "    # pad64:  Pads width and height with zeros to make them multiples of 64.\n",
    "    #         If IMAGE_MIN_DIM or IMAGE_MIN_SCALE are not None, then it scales\n",
    "    #         up before padding. IMAGE_MAX_DIM is ignored in this mode.\n",
    "    #         The multiple of 64 is needed to ensure smooth scaling of feature\n",
    "    #         maps up and down the 6 levels of the FPN pyramid (2**6=64).\n",
    "    # crop:   Picks random crops from the image. First, scales the image based\n",
    "    #         on IMAGE_MIN_DIM and IMAGE_MIN_SCALE, then picks a random crop of\n",
    "    #         size IMAGE_MIN_DIM x IMAGE_MIN_DIM. Can be used in training only.\n",
    "    #         IMAGE_MAX_DIM is not used in this mode.\n",
    "    IMAGE_RESIZE_MODE = \"crop\"\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    MEAN_PIXEL = np.array([107.0, 105.2, 101.5])\n",
    "    \n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 100\n",
    "    \n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 101\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 1000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between tree and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "    \n",
    "    # Weight decay regularization\n",
    "    WEIGHT_DECAY = 0.005\n",
    "\n",
    "    # Loss weights for more precise optimization.\n",
    "    # Can be used for R-CNN training setup.\n",
    "    LOSS_WEIGHTS = {\n",
    "        \"rpn_class_loss\": 1.0,\n",
    "        \"rpn_bbox_loss\": 1.0,\n",
    "        \"mrcnn_class_loss\": 1.0,\n",
    "        \"mrcnn_bbox_loss\": 1.0,\n",
    "        \"mrcnn_mask_loss\": 1.0,\n",
    "    }\n",
    "```\n",
    "\n",
    "##### Training Schedule\n",
    "\n",
    "**Training schedule changes**\n",
    "- Reduce Gaussian blur from 5.0 to 1.0\n",
    "- Increased 4+ and all layers # of epochs from 20 and 20 to 80 and 100, respectively.\n",
    "```python\n",
    "# Image augmentation\n",
    "    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "    if augmentation:\n",
    "        augmentation = iaa.SomeOf(\n",
    "            (0, 2),\n",
    "            [\n",
    "                iaa.Fliplr(0.5),\n",
    "                iaa.Flipud(0.5),\n",
    "                iaa.OneOf(\n",
    "                    [\n",
    "                        iaa.Affine(rotate=90),\n",
    "                        iaa.Affine(rotate=180),\n",
    "                        iaa.Affine(rotate=270),\n",
    "                    ]\n",
    "                ),\n",
    "                iaa.Multiply((0.8, 1.5)),\n",
    "                iaa.GaussianBlur(sigma=(0.0, 1.0)),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    print(\"Train network heads\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=20,\n",
    "                augmentation=augmentation,\n",
    "                layers='heads')\n",
    "    # Finetune layers from ResNet stage 4 and up\n",
    "    print(\"Fine tune Resnet stage 4 and up\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=100,\n",
    "                layers='4+')\n",
    "\n",
    "    print(\"Train all layers\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE/10,\n",
    "                epochs=200,\n",
    "                augmentation=augmentation,\n",
    "                layers='all')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff8138",
   "metadata": {},
   "source": [
    "### 10 Oct, 2022\n",
    "---\n",
    "\n",
    "#### Goals\n",
    "- Try to address the grappler `CropAndResize` error to see if that is causing issues\n",
    "- Add Grid Search (or maybe Random Search) capability to `tree.py`\n",
    "- Apply watershed to the output of `model_rgb.hdf5`\n",
    "- Watch some more of the Stanford course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d081307c",
   "metadata": {},
   "source": [
    "#### Grid Search plan\n",
    "- Use low # of epochs for coarse exploration\n",
    "- LR first\n",
    "- LR decay\n",
    "- Test anchor scales\n",
    "- Weight decay\n",
    "- Use Resnet101\n",
    "- Train only classifiers???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbe7e98",
   "metadata": {},
   "source": [
    "**Grid Search 1**\n",
    "- Update min conf to 0.6\n",
    "- Use resnet101\n",
    "- learning rates: 0.02, 0.0001, 0.00001\n",
    "- ~~lr decay: use custom callback with `ReduceLROnPlateau` (true, false)~~\n",
    "- anchor scales: (16, 32, 64, 128), (8, 16, 32, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b77d2",
   "metadata": {},
   "source": [
    "#### Run 4 (13:34)\n",
    "\n",
    "`python3 tree.py train --dataset=data/mrcnn --subset=hand --weights=coco --seed=1009 --gs=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b77d2",
   "metadata": {},
   "source": [
    "#### Run 5 (17:17)\n",
    "\n",
    "`python3 tree.py train --dataset=data/mrcnn --subset=hand --weights=coco --seed=1009 --gs=True`\n",
    "\n",
    "```python\n",
    "etas = [0.02, 0.001, 0.0001, 0.00001]\n",
    "anchor_scales = [(16, 32, 64, 128), (8, 16, 32, 64)]\n",
    "augmentation = None\n",
    "LRD = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31faa502",
   "metadata": {},
   "source": [
    "## 11 Oct, 2022\n",
    "\n",
    "### Goals\n",
    "- One more grid search with values that didn't get run\n",
    "- Run detection with proper config (based on GS results)\n",
    "- Apply watershed to semantic output\n",
    "- More Stanford\n",
    "- Consider Deep Forest detection instead\n",
    "- Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aad42e",
   "metadata": {},
   "source": [
    "### Run 6\n",
    "\n",
    "`python3 tree.py train --dataset=data/mrcnn --subset=hand --weights=coco --seed=1009 --gs=True`\n",
    "\n",
    "```python\n",
    "etas = [0.0001, 0.00001]\n",
    "anchor_scales = [(16, 32, 64, 128), (8, 16, 32, 64)]\n",
    "augmentation = None\n",
    "LRD = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d395881",
   "metadata": {},
   "source": [
    "### Watershed Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa76b0",
   "metadata": {},
   "source": [
    "Use labs/lab4/lusk_lab4-task4.py with RGB(I) weights file `model_rgb.hdf5`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dap05')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf136276df7b73f579d00ffc03546cf27d68eba258d4f2ea564dd7fb6a9dd1fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
