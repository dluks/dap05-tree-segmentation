{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9edee1-3afc-4908-ae42-c667286d2952",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization for Berlin Trees dataset\n",
    "\n",
    "- Author: Daniel Lusk\n",
    "- Date: 2022-09-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d617bf-b17d-4773-b5ab-16951bb72044",
   "metadata": {},
   "source": [
    "## Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad6fe3-58af-4c3b-ae6e-e3c45c8b3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tifffile as tiff\n",
    "from patchify import patchify\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    BatchNormalization,\n",
    "    Concatenate,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    Input,\n",
    "    MaxPool2D,\n",
    ")\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def patch_train_label(raster, labels, img_size, channels=False, merge_channel=False):\n",
    "    samp_rast = tiff.imread(raster[0])\n",
    "    img_base_size = samp_rast.shape[0]\n",
    "    n = len(raster)\n",
    "    m = (img_base_size // img_size) ** 2\n",
    "\n",
    "    if not channels:\n",
    "        channels = samp_rast.shape[-1]\n",
    "\n",
    "    if merge_channel:\n",
    "        channels += tiff.imread(merge_channel[0]).shape[-1]\n",
    "\n",
    "    data_train = np.zeros((n * m, img_size, img_size, channels))\n",
    "    data_label = np.zeros((n * m, img_size, img_size))\n",
    "\n",
    "    for k in range(n):\n",
    "        if merge_channel:\n",
    "            r = np.concatenate(\n",
    "                (tiff.imread(raster[k]), tiff.imread(merge_channel[k])), axis=-1\n",
    "            )\n",
    "        else:\n",
    "            r = tiff.imread(raster[k])[..., :channels]\n",
    "\n",
    "        # Only read in the specified number of channels from input raster\n",
    "        patches_train = patchify(\n",
    "            r,\n",
    "            (img_size, img_size, channels),\n",
    "            step=img_size,\n",
    "        )\n",
    "        patches_label = patchify(\n",
    "            tiff.imread(labels[k]), (img_size, img_size), step=img_size\n",
    "        )\n",
    "        data_train[k * m : (k + 1) * m, :, :, :] = patches_train.reshape(\n",
    "            -1, img_size, img_size, channels\n",
    "        )\n",
    "        data_label[k * m : (k + 1) * m, :, :] = patches_label.reshape(\n",
    "            -1, img_size, img_size\n",
    "        )\n",
    "\n",
    "    data_label = (data_label > 0).astype(\"int\")\n",
    "    data_label = np.expand_dims(data_label, axis=-1)\n",
    "    data_train = data_train.astype(\"float\") / 255\n",
    "\n",
    "    print(\n",
    "        f\"\\nData sizes:\\ndata_train: {data_train.shape}\\ndata_label: {data_label.shape}\\n\"\n",
    "    )\n",
    "\n",
    "    return data_train, data_label\n",
    "\n",
    "\n",
    "# Construct the U-Net\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "    b1 = conv_block(p4, 1024)\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_unet(x_train, y_train, x_test, y_test, batch_size, epochs, eta):\n",
    "    input_shape = x_train.shape[1:]\n",
    "\n",
    "    model = build_unet(input_shape)\n",
    "    batch_size = batch_size\n",
    "    epochs = epochs\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=eta),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c940224-ad4c-492f-9d0d-b274e3593b21",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8213488f-1dd9-4b91-88fb-906ca9e8f2ac",
   "metadata": {},
   "source": [
    "### Edit these values\n",
    "These are the directories your hand-labeled RGB, NIR, and labels tifs live in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd02384-92ae-4d53-a73c-5607ae3790b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit me!\n",
    "data_dir = \"../../../data/\"  # Your base directory for the RGB, NIR, and labels\n",
    "hand_rgb_dir = f\"{data_dir}train_rgb/\"  # Subdirectory for RGB\n",
    "hand_nir_dir = f\"{data_dir}train_nir/\"  # Subdirectory for NIR\n",
    "hand_label_dir = f\"{data_dir}label/\"  # Subdirectory for labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d75bc70-53f5-4439-a8ee-3336c6e45bc7",
   "metadata": {},
   "source": [
    "Patchify the hand-labeled data plus NIR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103932e0-0f42-43c1-9b0e-26b66e4e52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patchify hand-labeled data PLUS NIR data\n",
    "patch_rgb = glob.glob(f\"{hand_rgb_dir}*.tif\")\n",
    "patch_nir = glob.glob(f\"{hand_nir_dir}*.tif\")\n",
    "patch_label = glob.glob(f\"{hand_label_dir}*.tif\")\n",
    "\n",
    "patch_rgb.sort()\n",
    "patch_label.sort()\n",
    "\n",
    "print(\"Patchifying RGB + NIR data...\")\n",
    "data_train, data_label = patch_train_label(\n",
    "    patch_rgb, patch_label, 128, merge_channel=patch_nir\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160615a-ced2-49bf-bc37-52d6844f75f0",
   "metadata": {},
   "source": [
    "### Edit these values, too\n",
    "\n",
    "These are the directories that your PRE-PATCHIFIED watershed RGBI and labeled images live in. These images should already be patchified into 512x512 patches before we patchify them *again* into 128x128 patches below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680bc2a-2af3-4a14-ad58-039b00723ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit me!\n",
    "\n",
    "# Your base directory for the watershed patched RGBI and labels\n",
    "ws_data_dir = \"../../../data/\"\n",
    "# Subdirectory for RGBI\n",
    "patched_watershed_rgbi_dir = f\"{ws_data_dir}watershed/512/rgbi/\"\n",
    "# Subdirectory for labels\n",
    "patched_watershed_label_dir = f\"{ws_data_dir}watershed/512/labels/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f55942-b668-4de3-8cc6-c894ad0f0a26",
   "metadata": {},
   "source": [
    "Patchify the pre-patchified watershed images and labels, and then combine all the data into `x_train`, `x_test`, `y_train`, and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85cc1e-a88f-447d-be41-ac75e802f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patchify watershed data (pre-patchified)\n",
    "watershed_rgbi = glob.glob(f\"{patched_watershed_rgbi_dir}*.tif\")\n",
    "watershed_labels = glob.glob(f\"{patched_watershed_label_dir}*.tif\")\n",
    "\n",
    "watershed_rgbi.sort()\n",
    "watershed_labels.sort()\n",
    "\n",
    "print(\"Patchifying watershed data...\")\n",
    "data_train_ws, data_label_ws = patch_train_label(watershed_rgbi, watershed_labels, 128)\n",
    "\n",
    "data_train = np.vstack((data_train, data_train_ws))\n",
    "data_label = np.vstack((data_label, data_label_ws))\n",
    "\n",
    "print(\n",
    "    f\"\\nSizes after adding watershed data:\\n\\\n",
    "data_train: {data_train.shape}\\n\\\n",
    "data_label: {data_label.shape}\\n\"\n",
    ")\n",
    "\n",
    "# Shuffle the data\n",
    "rng = np.random.default_rng(seed=42)\n",
    "n = len(data_train)\n",
    "k = rng.choice(n, size=n, replace=False)\n",
    "data_train = data_train[k]\n",
    "data_label = data_label[k]\n",
    "\n",
    "# Split into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    data_train, data_label, test_size=0.1, random_state=157\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nSizes after splitting data:\\n\\\n",
    "x_train: {x_train.shape}\\n\\\n",
    "y_train: {y_train.shape}\\n\\\n",
    "x_test: {x_test.shape}\\n\\\n",
    "y_test: {y_test.shape}\"\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be163e-0490-4682-925c-81d8597f3372",
   "metadata": {},
   "source": [
    "## Run the grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0bdac6-61c9-40f9-9edc-52ea7bc3a8a5",
   "metadata": {},
   "source": [
    "### And now you can run the hyperparameter grid search (no need to change anything else)\n",
    "\n",
    "This will take a while (i.e. hours, maybe a day), so only do this when you're ready to leave the computer for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b3347-49ff-440c-9eb9-20e01a056597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRID SEARCH SET ONE\n",
    "set_name = \"batch_16_32_eta_0-01_0-02\"\n",
    "batch_sizes = np.array([16, 32])\n",
    "etas = np.array([1e-2, 1e-2])\n",
    "\n",
    "# GRID SEARCH SET TWO\n",
    "# set_name = \"batch_16_32_eta_0-05_0-1\"\n",
    "# batch_sizes = np.array([16, 32])\n",
    "# etas = np.array([5e-2, 1e-1])\n",
    "\n",
    "# Data structure for future grid search data storage\n",
    "n_folds = 2\n",
    "data = np.zeros((n_folds, batch_sizes.size, etas.size, 5), dtype=object)\n",
    "\n",
    "# Initialize the KFold\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=7)\n",
    "\n",
    "# %% Run reduced grid search\n",
    "epochs = 1\n",
    "\n",
    "for i, (itrain, itest) in enumerate(\n",
    "    tqdm(\n",
    "        kf.split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "        ),\n",
    "        desc=\"K-Folds\",\n",
    "        position=0,\n",
    "        leave=False,\n",
    "    )\n",
    "):\n",
    "    x_train_fold = x_train[itrain]\n",
    "    y_train_fold = y_train[itrain]\n",
    "    x_test_fold = x_train[itest]\n",
    "    y_test_fold = y_train[itest]\n",
    "\n",
    "    for j, batch_size in enumerate(\n",
    "        tqdm(batch_sizes, desc=\"batch size\", position=1, leave=False)\n",
    "    ):\n",
    "        for k, eta in enumerate(tqdm(etas, desc=\"ETA\", position=2, leave=False)):\n",
    "            # Run U-Net Here\n",
    "            model, history = train_unet(\n",
    "                x_train_fold,\n",
    "                y_train_fold,\n",
    "                x_test_fold,\n",
    "                y_test_fold,\n",
    "                batch_size,\n",
    "                epochs,\n",
    "                eta,\n",
    "            )\n",
    "\n",
    "            # Loss and accuracies from each epoch\n",
    "            loss = history.history[\"loss\"]\n",
    "            val_loss = history.history[\"val_loss\"]\n",
    "            iou = list(history.history.keys())[1]\n",
    "            val_iou = list(history.history.keys())[3]\n",
    "\n",
    "            # Test the model on the preserved test data\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "            # Convert sigmoid probability to classification\n",
    "            y_pred_thresholded = y_pred > 0.5\n",
    "\n",
    "            # Get the IoU for the test data\n",
    "            IOU_keras = MeanIoU(num_classes=2)\n",
    "            IOU_keras.update_state(y_pred_thresholded, y_test)\n",
    "            pred_iou = IOU_keras.result().numpy()\n",
    "\n",
    "            # Log the five stats according to their K-Fold and parameter iteration\n",
    "            stats = [pred_iou, loss, val_loss, iou, val_iou]\n",
    "\n",
    "            for s, stat in enumerate(stats):\n",
    "                data[i, j, k, s] = stat\n",
    "\n",
    "np.save(f\"nfolds_{n_folds}_{set_name}.npy\", data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lusk_dap05",
   "language": "python",
   "name": "lusk_dap05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
